### 树回归
- 优点: 可以对复杂和非线性的数据建模
- 缺点: 结果不易理解
- 适用数据类型: 数值型和标称型数据
### 树回归的一般方法
1. 收集数据: 采用任意方法收集数据
2. 准备数据: 需要数值型的数据,标称型数据应该映射成二值性数据
3. 分析数据: 绘出数据的二维可视化显示结果,以字典方式生成树
4. 训练算法: 大部分时间都花费在叶节点树模型的构建上
5. 测试算法:使用测试数据上的$R^2$值来分析模型的效果 
6. 使用算法:使用训练出来的树做预测,预测结果还可以用来做很多事情
### createTree()伪代码
    找到最佳的待切分特征:
        如果该节点不能再分,将该节点存为叶节点
        执行二元切分
        在右子树调用createTree()方法
        在左子树调用createTree()方法 
### 将CART算法用于回归
- 为了成功构建以分段常数为叶节点的树,树妖都两处数据的一致性.使用数进行分类,会在给定节点时计算数据的混乱度.事实上,在数据集上计算混乱度时非常简单的.首先计算所有数据的均值,然后计算每条数据的值到均值的差值.为了对正负差值同等看待,一般使用绝对值或平方值来代替上述差值.
- 均方差是平方误差的均值.总方差是平方误差的总值,总方差可以通过均方差乘以数据集中样本点的个数来得到
### 构建树
- 函数chooseBestSplit()函数便利所有的特征及其可能的取值来找到使误差最小化的切分阈值,该函数伪代码如下
        对每个特征:
            对每个特征值:
                将数据集切分成两份
                计算切分的误差
                如果当前误差小于当前最小误差,那么将当前切分设定为最佳切分并更新最小误差
        返回最佳切分的特征和阈值