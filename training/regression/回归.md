### 回归的一般方法
1. 收集数据: 采用任意方法收集数据
2. 准备数据: 回归需要数值型数据,标称型数据将被转成二值型数据
3. 分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析,在采用缩减法求得新回归系数之后,可以将新拟合线绘在图上作为对比
4. 训练算法: 找到回归系数
5. 测试算法: 使用$R^2$或者预测值和数据的拟合度,来分析模型的效果
6. 使用算法: 使用回归,可以在给定输入的时候预测出一个数值,这是对分类方法的提升,因为这样可以预测连续型数据而不仅仅是离散型的类别标签
- 假定输入数据存放在矩阵x中,而回归系数存放在向量w中.那么对于给定的数据$x_1$,预测结果将会通过$Y_1=X_1^{T}w$给出.假定现在有一些x和对应的y,可以找出使误差最小的w.这里的误差是指预测y值和真实y值之间的差值,使用该误差的简单累加将使得正差值和负差值相互抵消,因此采用平方误差
- **平方误差: **$\sum_{i=1}^{m}(y_i-x_{i}^{T}w)^2$
$用矩阵表示还可以写作(y-Xw)^{T}(y-Xw).如果对w求导,得到x^T(Y-Xw),令其等于零,解出w如下:\\ \hat{w}=\left(X^{T}X\right)^{-1}X^{T}y$
- $求导: f(x)=yx^{2}+x+y+xy\\ fx(x,y)=2xy+1+y\\ fy(x,y)=x^{2}+1+x$
- $\hat{w}表示这是当前可以估计出的w的最优解$
### 局部加权线性回归(LWLR)
- $\hat(w)=(X^{T}WX)^{-1}X^{T}Wy,\text{W是一个矩阵,用来给每个数据点赋予权重}$
- LWLR使用"核"(与SVM中的kernel类似)来对附近的点赋予更高的权重.核的类型可以自由选择,最常用的核就是高斯核,高斯核对应的权重如下:
$w(i,i)=\exp\left(\dfrac{\mid{x^{i}-x}\mid}{-2k^{2}}\right)$