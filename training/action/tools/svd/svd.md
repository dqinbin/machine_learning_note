## 内容概要
    SVD矩阵分解
    推荐引擎
    利用SVD提升推荐引擎的性能
#### SVD的应用
                    奇异值分解
    优点: 简化数据,去除噪声,提高算法的结果
    缺点: 数据的转换可能难以理解
    适用数据类型: 数值型数据
- 利用SVD实现,可以用较小的数据集来表示原始数据集.这样做,实际上是去除了噪声和冗余信息
#### 隐性语义索引
- 最早的SVD应用之一就是信息检索.利用SVD的方法被称为**隐性语义索引(Latent Semantic Indexing,LSI)**或**隐性语义分析(Latent Semantic Analysis,LSA)**
- 在LSI中,一个矩阵是由文档和词语组成的.当我们在该矩阵上应用SVD时,就会构建出多个奇异值.这些奇异值代表了文档中的概念或主题,这一特点可以用于更高效的文档搜索.在词语拼写错误时,只基于词语存在与否的简单搜索方法会遇到问题.简单搜索的另一个问题就是同义词的使用.也就是说,当我们查找一个词时,其同义词所在的文档可能并不会匹配上.如果我们从上千篇相似的文档中抽取出概念,那么同义词就会映射为同一概念
#### 推荐系统
- SVD的另一个应用就是推荐系统.简单版的推荐系统能够计算项或者人之间的相似度.更先进的方法则利用SVD从数据中构建一个主题空间,然后再在该空间下计算其相似度
#### 矩阵分解
- 在线代数中有很多矩阵分解技术.矩阵分解可以将原始矩阵表示成新的已于处理的形式,这种形式是两个或多个矩阵的乘积.我们可以将这种分解过程想象成代数中的因子分解
- 不同的矩阵分解技术具有不同的性质,其中有些更适合于某个应用,有些则更适合于其他应用.最常见的一种矩阵分解技术就是SVD.SVD将原始的数据集矩阵Data分解成三个矩阵U,$\Sigma$和$V^T$.如果原始矩阵Data是m行n列,那么U,$\Sigma$和$V^T$就分别是m行m列、m行n列和n行n列:\
    $Data_{m\times{n}}=U_{m\times{m}}\Sigma_{m\times{n}}V_{n\times{n}}^T$\
    上述分解中会构建出一个矩阵$\Sigma$,该矩阵只有对角元素,其他元素均为0.另一个惯例就是,$\Sigma$的对角元素是从大到小排列的.这些对角元素称为**奇异值(Singular Value)**,它们对应了原始数据集矩阵Data的奇异值.在PCA中,计算的是矩阵的特征值,它们噶苏我们数据集中的重要特征.$\Sigma$中的奇异值也是如此.奇异值和特征值是有关系的.这里的奇异值就是矩阵$Data\times{Data^T}$特征值的平方根
- 之前提到过,矩阵$\Sigma$只有从大到小排列的对角元素.在科学和工程中,一直存在这样一个普遍事实:在某个奇异值的数目(r个)之后,其他的奇异值都置为0.这就意味着数据集中仅有r个重要特征,而其余特征则都使噪声或冗余特征
#### 利用Python实现SVD
- 确定要保留的奇异值的数目有很多启发式的策略,其中一个典型的做法就是保留矩阵中90%的能量信息.为了计算总能量信息我们将所有奇异值求平方和.于是可以将奇异值的平法和累加到总之的90%以上.另一个启发式策略就是,当矩阵上有上万的奇异值时,那么就保留前面的2000或3000个
#### 相似度计算
- 第一种计算方式是通过欧氏距离公式计算物品之间的距离.我们希望,相似度在0到1之间变化,并且物品对越相似,它们相似度值也就越大.可以用"相似度=1(1+距离)"这样的算式来计算相似度.当距离为0时,相似度为1.0.如果距离真的非常大时,相似度也就趋近于0
- 第二种计算距离的方法是**皮尔逊相关系数(Pearson correlation)**.它度量的是两个向量之间的相似度.对比于欧氏距离的优势在于,它对用户评级的量级并不敏感.在numpy中,皮尔逊相关系数的计算由函数corrcoef()进行的.皮尔逊相关系数的取值范围从-1到+1,通过0.5+0.5*corrcoef()这个函数计算,并且把取值范围归一化到0到1之间
- 另一个常用的距离计算方法就是**余弦相似度(cosine similarity)**.其计算的是两个向量夹角的余弦值.如果夹角度为90度,则相似度为0;如果两个向量的方向相同,则相似度为1.0.同皮尔逊相关系数一样,余弦相似度的取值范围也在-1到+1之间,因此也将它归一化到0到1之间.计算余弦相似度值,采用的两个向量A和B夹角的余弦相似度的定义如下:
    $\cos\theta=\dfrac{A\times{B}}{\parallel{A}\parallel \parallel{B}\parallel}$
    $其中,\parallel{A}\parallel,\parallel{b}\parallel表示向量A,B的2范数,可以定义向量的任意范数,但是如果不指定范数阶数,则都假设为2范数,向量[4,2,2]的2范数为:\sqrt[2]{4^2+2^2+2^2}$
    同样,Numpy的现行代数工具箱中提供了范数的计算方法linalg.norm()
#### 案例:推荐未尝过的菜肴
    推荐系统的工作过程是:给定一个用户,系统会为此用户发明会N个最好的推荐菜.为了实现这一点,我们需要做到:
    1.寻找用户没有评级的菜肴,即在用户-物品矩阵中的0值
    2.在用户没有评级的所有物品中,对每个物品预计一个可能的评级分数.这就是说,我们认为用户可能会对物品的打分(这就是相似度计算的初衷)
    3.对这些物品的评分从高到低进行排序,返回前N个物品