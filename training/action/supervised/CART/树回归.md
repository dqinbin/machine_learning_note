### 树回归
- 优点: 可以对复杂和非线性的数据建模
- 缺点: 结果不易理解
- 适用数据类型: 数值型和标称型数据
### 树回归的一般方法
1. 收集数据: 采用任意方法收集数据
2. 准备数据: 需要数值型的数据,标称型数据应该映射成二值性数据
3. 分析数据: 绘出数据的二维可视化显示结果,以字典方式生成树
4. 训练算法: 大部分时间都花费在叶节点树模型的构建上
5. 测试算法:使用测试数据上的$R^2$值来分析模型的效果 
6. 使用算法:使用训练出来的树做预测,预测结果还可以用来做很多事情
### createTree()伪代码
    找到最佳的待切分特征:
        如果该节点不能再分,将该节点存为叶节点
        执行二元切分
        在右子树调用createTree()方法
        在左子树调用createTree()方法 
### 将CART算法用于回归
- 为了成功构建以分段常数为叶节点的树,树妖都两处数据的一致性.使用数进行分类,会在给定节点时计算数据的混乱度.事实上,在数据集上计算混乱度时非常简单的.首先计算所有数据的均值,然后计算每条数据的值到均值的差值.为了对正负差值同等看待,一般使用绝对值或平方值来代替上述差值.
- 均方差是平方误差的均值.总方差是平方误差的总值,总方差可以通过均方差乘以数据集中样本点的个数来得到
### 构建树
- 函数chooseBestSplit()函数便利所有的特征及其可能的取值来找到使误差最小化的切分阈值,该函数伪代码如下

    ​

        对每个特征:
        对每个特征值:
          将数据集切分成两份
          计算切分的误差
          如果当前误差小于当前最小误差,那么将当前切分设定为最佳切分并更新最小误差
        返回最佳切分的特征和阈值
    ​
### 后剪枝
- 思路:需要将数据集分成测试集和训练集.首先指定参数,使构建出的树足够大,足够复杂,便于剪枝.接下来从上而下找到叶节点,用测试集来判断将这些叶节点合并是否嗯那个降低测试误差,如果是就合并
  **伪代码如下**

      基于已有的树切分测试数据:  
        如果存在任一子集使一棵树,则在该子集递归剪枝过程
        计算将当前两个叶节点合并后的误差
        计算不合并的误差
        如果合并会降低误差的话,就将叶节点合并
### 模型树,树回归比较
- 一个比较客观的方法是计算相关系数,也称$R^2$值.该相关系数可以通过调用Numpy库中的命令corrcoef(yHat,y,rowvar=0)来求解,其中yHat是预测值,y是目标变量的实际值