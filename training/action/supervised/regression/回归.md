### 回归的一般方法
1. 收集数据: 采用任意方法收集数据
2. 准备数据: 回归需要数值型数据,标称型数据将被转成二值型数据
3. 分析数据: 绘出数据的可视化二维图将有助于对数据做出理解和分析,在采用缩减法求得新回归系数之后,可以将新拟合线绘在图上作为对比
4. 训练算法: 找到回归系数
5. 测试算法: 使用$R^2$或者预测值和数据的拟合度,来分析模型的效果
6. 使用算法: 使用回归,可以在给定输入的时候预测出一个数值,这是对分类方法的提升,因为这样可以预测连续型数据而不仅仅是离散型的类别标签
- 假定输入数据存放在矩阵x中,而回归系数存放在向量w中.那么对于给定的数据$x_1$,预测结果将会通过$Y_1=X_1^{T}w$给出.假定现在有一些x和对应的y,可以找出使误差最小的w.这里的误差是指预测y值和真实y值之间的差值,使用该误差的简单累加将使得正差值和负差值相互抵消,因此采用平方误差
- **平方误差: **$\sum_{i=1}^{m}(y_i-x_{i}^{T}w)^2$
  $用矩阵表示还可以写作(y-Xw)^{T}(y-Xw).如果对w求导,得到x^T(Y-Xw),令其等于零,解出w如下:\\ \hat{w}=\left(X^{T}X\right)^{-1}X^{T}y$
- $求导: f(x)=yx^{2}+x+y+xy\\ fx(x,y)=2xy+1+y\\ fy(x,y)=x^{2}+1+x$
- $\hat{w}表示这是当前可以估计出的w的最优解$
### 局部加权线性回归(LWLR)
- $\hat(w)=(X^{T}WX)^{-1}X^{T}Wy,\text{W是一个矩阵,用来给每个数据点赋予权重}$
- LWLR使用"核"(与SVM中的kernel类似)来对附近的点赋予更高的权重.核的类型可以自由选择,最常用的核就是高斯核,高斯核对应的权重如下:
  $w(i,i)=\exp\left(\dfrac{\mid{x^{i}-x}\mid}{-2k^{2}}\right)$
### 岭回归和逐步回归
- **问题：** 如果特征比样本点还多(n>m),即输入数据的矩阵X不是满秩矩阵.非满秩矩阵求逆时会出现问题.为了解决这个问题,引入了**岭回归**与**逐步回归**的概念
- **岭回归求回归系数:** $\hat{w}=(X^{T}X+\lambda{I})^{-1}X^{T}y,\text{引入$\lambda$惩罚项,能够减少不重要的参数--缩减(shrinkage)}$
- **岭回归中的岭:** 岭回归使用了单位矩阵乘以常量$\lambda$,我们观察其中的单位矩阵I,可以看到值l贯穿整个对角线,其余元素全是0,形象的,在0构成的平面上有i一条l组成的'岭',这就是岭回归中的岭的由来
- **前向逐步回归:** 属于一种贪心算法,即每一步都尽可能减少误差,一开始的权重都设为1,然后每一步所做的决策是对某个权重增加或减少一个很小的值,该算法的伪代码如下:
        数据标准化,使其分布满足0均值和单位方差
        在每轮迭代过程中:
        设置当前最小误差lowestError为正无穷
        对每个特征:
            增大或减小:
                改变一个系数得到一个新的W
                计算新W下的误差
                如果误差Error小于当前最小误差lowestError:设置Wbest等于当前的W
            将当前的W设置为新的Wbest 
